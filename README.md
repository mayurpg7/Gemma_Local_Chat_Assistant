# Gemma Local Chat Assistant

A fully **local AI chat assistant** powered by **Google’s Gemma 2B Instruct** model — built and executed entirely on **Google Colab**, with zero paid APIs or external servers.  
This project showcases how to run a large-language model locally using open-source tools.

---

## 🚀 Overview

This project allows you to chat with **Gemma**, Google’s lightweight open-source LLM, through a **Gradio web interface**.  
It uses `llama-cpp-python` for inference and the **Hugging Face Hub** for model access.

> ⚡ *Runs slower on free Colab hardware, but is completely free and private!*

---

## 🧠 Features

- 💬 **Interactive Chat UI** powered by Gradio  
- ⚙️ **100 % Local Inference** — no paid API keys required  
- 🔐 **Secure Hugging Face Token Authentication**  
- 🧩 **Lightweight & Free** — optimized for Google Colab  
- 🧱 **Educational** — learn how local inference works step by step  

---

## 🧰 Tech Stack

| Component | Purpose |
|------------|----------|
| **Python 3** | Core programming language |
| **Gradio** | Web-based chat interface |
| **llama-cpp-python** | Local model inference engine |
| **Hugging Face Hub** | Model storage & authentication |
| **Google Colab** | Free cloud runtime |

---

🕒 Performance Note
This project runs entirely in Google Colab using free CPU/GPU.
Expect slower responses compared to cloud-hosted APIs, but enjoy zero cost, privacy, and full transparency.

🧑‍💻 By:
Mayur Prakash Gaikwad
🎓 Data & AI Enthusiast | Business Analyst | Engineer
🌐 Built with ❤️ using free tools and smart engineering.
