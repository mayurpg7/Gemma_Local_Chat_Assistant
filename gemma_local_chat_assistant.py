# -*- coding: utf-8 -*-
"""Gemma Local Chat Assistant.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L4OMj1X_VrvAg45JWwmWr39vH90mF5C0
"""

!pip install -U llama-cpp-python huggingface_hub gradio pyngrok gtts --quiet

!pip install -U "huggingface-hub<1.0"

from huggingface_hub import login
import os

login(token=os.getenv("HF_TOKEN"))

from llama_cpp import Llama

llm = Llama.from_pretrained(
    repo_id="google/gemma-2b-it",
    filename="gemma-2b-it.gguf",
    n_ctx=2048,   # context length
    n_gpu_layers=-1  # run on GPU if available
)

import gradio as gr

def chat_with_gemma(message, history):
    # Combine previous messages for context
    conversation = ""
    for human, ai in history:
        conversation += f"User: {human}\nAI: {ai}\n"
    conversation += f"User: {message}\nAI:"

    # Run inference
    response = llm(conversation, max_tokens=256)
    reply = response["choices"][0]["text"].strip()
    return reply

# Create Gradio Chat Interface
chatbot = gr.ChatInterface(fn=chat_with_gemma, title="ðŸ’« Gemma Local Chat Assistant")
chatbot.launch(share=True)







